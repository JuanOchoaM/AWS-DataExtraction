# AWS-DataExtraction
I utilized Google Cloud Platform (GCP) and Hadoop to analyze AWS Spot Instances pricing data. The project focused on identifying pricing variability and expensive instance types within a region.

1. **GCP Setup**: Created an Ubuntu VM instance on GCP, configured firewall rules, and set up the environment for Hadoop.

2. **Hadoop Installation**: Installed and configured Hadoop on the GCP instance, including setting up necessary directories, users, and SSH keys for Hadoop and YARN.

3. **Data Processing with Hadoop**: Used Hadoop to run MapReduce programs for analyzing AWS pricing data. Identified the top three instance types with the highest price variability and the most expensive instance types within specified timeframes.

4. **Visualization**: Generated plots (Microsoft Excel) showing average prices of the most expensive instance types as a function of time-of-day, using the data processed by Hadoop.

### Skills Gained:
- Experience with Google Cloud Platform services, including VM setup and firewall configuration.
- Proficiency in installing and configuring Hadoop on a cloud-based VM.
- Practical knowledge of running MapReduce programs for data analysis.
- Skills in visualizing data results using external tools like Excel.

This project enhanced my ability to set up and utilize cloud resources for big data processing, leveraging both Google Cloud Platform and Hadoop for efficient data analysis.
